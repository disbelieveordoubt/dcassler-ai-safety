# AI Safety Research: Conversational Vulnerability Testing

**Daniel Cassler** | dcassler@gmail.com | linkedin.com/in/danielvermont

## Summary

I've developed a testing methodology that identifies architectural 
vulnerabilities in AI systems through conversational interrogation. 
This approach forces models to demonstrate and admit their own 
reasoning failures in real-time.

## Key Finding

Using child protection compliance standards as a baseline, I've 
documented systematic failures across all frontier models (Claude, 
GPT, DeepSeek, voice assistants). The pattern is consistent: 
models apply inconsistent standards, recognize the failure when 
challenged, but cannot prevent recurrence.

## Evidence

The `conversation-logs/` directory contains 5 demonstrations showing:
- Initial failure (inconsistent reasoning)
- Socratic challenge (forcing recognition)
- Meta-cognitive admission (model acknowledges the failure)
- Architectural persistence (failure recurs despite awareness)

## Background

15 years operational security experience including threat analysis 
for platforms with 55M+ users and critical infrastructure networks.

## Contact

Available for demonstration and discussion of testing methodology.

---
*Research conducted independently, August-December 2025*